\section{Discussion} \label{Discussion} 

Multiple studies have been published in the past reviewing the state of \gls{DL} and \gls{ML} for classifying \gls{AD} \autocite[see][]{ebrahimighahnaviehDeepLearningDetect2020,joDeepLearningAlzheimer2019, tanveerMachineLearningTechniques2020, wenConvolutionalNeuralNetworks2020, noorApplicationDeepLearning2020}. Most of these reviews focus on \gls{DL} methods, which may be explained by the increasing trend to use \gls{DL} model for detection of \gls{AD} \autocite{ebrahimighahnaviehDeepLearningDetect2020}, as well as its ability to achieve high accuracy on high-dimensional and complex data \autocite{tanveerMachineLearningTechniques2020, lecunDeepLearning2015a}. Furthermore, as cited in \textcite{lecunDeepLearning2015a}, \gls{DL} models  outperformed other models in multiple areas such as image and speech recognition. Especially \gls{cnn} models are shown to have good performance in the medical field \autocite{ebrahimighahnaviehDeepLearningDetect2020}. \gls{svm} on the other hand, are still often used for \gls{AD} classification because of their robustness, interpretability \autocite{tanveerMachineLearningTechniques2020}, are easier to validate and can be trained on a smaller data set \autocite{syaifullahMachineLearningDiagnosis2021}. 
Considering the \textit{no free lunch theorem}, which states that there is no best model that is superior to other models independently of data set used \autocite{introtostat, deeplearningav}, this review focuses on both approaches: \gls{svm} and \gls{cnn}. 
Only studies published in the year 2020 and 2021 were considered for this review since most earlier studies have already been reviewed \autocite[see][]{ebrahimighahnaviehDeepLearningDetect2020,joDeepLearningAlzheimer2019, tanveerMachineLearningTechniques2020, wenConvolutionalNeuralNetworks2020, noorApplicationDeepLearning2020}. A total of 10 studies, with five using \gls{svm} and five using \gls{cnn}, were included in this paper. Tables \ref{tab:ml} and \ref{tab:dl} provide an overview of the studies included.  

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Studies using SVM}

An extensive review by \textcite{tanveerMachineLearningTechniques2020} found that out of 60 studies using \gls{svm} the most frequently applied kernel were: linear (\textit{43\%}) and \gls{rbf} (\textit{32\%}). \gls{loocv} (\textit{43\%}) and 10 fold cross-validation (\textit{30\%}) were the most often used validation methods. They reported accuracy for the binary classification task varying between 72.5 and 100\% with most study achieving 90\% or higher. 

In this review, three studies used a linear kernel while only one study used a \gls{rbf} kernel. Two of the studies that applied linear kernel also analysed their models using \gls{rbf} as kernel and found worse \autocite{khatriEfficientCombinationSMRI2020a} or equal \autocite{akramifardEmphasisLearningFeatures2020} performance. 10-fold cross validation was the most often used (3 out of 5 studies) validation method in this review and LOOCV was only used by one study \autocite{syaifullahMachineLearningDiagnosis2021}. All reviewed studies used the \gls{adni} data set for training and only two studies tested their model on different databases; \textcite{syaifullahMachineLearningDiagnosis2021} used four different databases (\gls{aibl}, Japan \gls{adni}, \gls{miriad}, \gls{oasis}) and \textcite{liuMultimodelDeepConvolutional2020a} used an own data set consisting of subjects scanned at Ruijin Hospital in China. The authors of that study claim that this data set provides additional cross-racial validation since their model was trained using \gls{adni} where the subjects are from North America \autocite{weiner2013alzheimer}. Although \textcite{richhariyaDiagnosisAlzheimerDisease2020} used an independent data set for testing their model it was still the same database \gls{adni}. Since all \gls{mri} images from \gls{adni} are made under the same protocol \autocite{jack2008alzheimer}, one can argue to what degree testing the model on the same database would reflect clinical practice. 

Three studies used additional modalities as feature input for their model. \textcite{akramifardEmphasisLearningFeatures2020} used personal information (for example age and gender), \gls{mmse}, \gls{csf} and \gls{pet} data. A further analysis made by the author showed that their model using \gls{mmse} score alone could classify AD and NC with an accuracy of \textit{91.9\%} while \gls{mri} data on its own achieved an accuracy of \textit{86.6\%}. A possible explanation for MMSE scores achieving high accuracy could stem from the inclusion criteria for AD subjects in the \gls{adni} study. One criterion being a MMSE scores between 20 and 26 (inclusive), where as the MMSE inclusion criterion of \gls{nc} subjects in the \gls{adni} study is a score between 24 and 30 (inclusive) \autocite{Petersen201}. Therefore, MMSE scores can distinguish AD from NC with high accuracy. Furthermore, a possible explanation for why the accuracy isn't \textit{100\%} could stem from the fact that in the ADNI study, inclusion criterion for MMSE scores of AD and NC overlap. It is likely that this was the case in the study by \textcite{akramifardEmphasisLearningFeatures2020}, since they report an average MMSE score of \textit{23.32} in their AD subjects and \textit{27.05} in NC subjects.


The fact that all studies reviewed used \gls{roi} as input reflects \gls{svm}s limitation of performing badly on raw data and the need for pre-processing and feature extraction \autocite{vieiraUsingDeepLearning2017}. 
The overall performance metrics of the reviewed studies are very high with lowest accuracy of \textit{84.85\%} and highest \textit{100\%}. The studies that used independent test set all showed over fitting since their performance decreased. One exception was \textcite{syaifullahMachineLearningDiagnosis2021}. While testing on the independent data base \gls{miriad}, the authors found an increase in accuracy (\textit{93.30\%} to \textit{94.20\%}) and sensitivity (\textit{93.30\%} to \textit{97.80\%}) but specificity dropped from \textit{93.40\%} to \textit{87.00\%}. Finally, the absence of an independent test set in \textcite{akramifardEmphasisLearningFeatures2020} may have led to data leakage as well as making it nontransparent to what degree their model is over fitted.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Studies using CNN}
In the past years, multiple reviews on \gls{AD} classification using \gls{DL} have been published \autocite[see][]{wenConvolutionalNeuralNetworks2020,ebrahimighahnaviehDeepLearningDetect2020, joDeepLearningAlzheimer2019,tanveerMachineLearningTechniques2020,noorApplicationDeepLearning2020}. They all concluded that \gls{cnn} are the most widely used model for this classification task with T1-weighted \gls{mri} being the most used modality \autocite{tanveerMachineLearningTechniques2020}. Although \gls{cnn} models can perform well on raw data \autocite{lecunDeepLearning2015a} and is seen as an advantage over SVM models \autocite{joDeepLearningAlzheimer2019}, pre-processing steps and input management are widely used with \gls{cnn} models \autocite{ebrahimighahnaviehDeepLearningDetect2020}. In this review all studies included some sort of pre-processing. \textcite{wenConvolutionalNeuralNetworks2020} varied the amount of pre-processing and analysed its effect on performance showing that \acrlong{in} is a crucial step in \gls{cnn}. \textcite{ebrahimighahnaviehDeepLearningDetect2020} finding that \gls{in} is the most often used pre-processing technique reflects this importancy. \gls{in} is needed since \gls{mri} scanners can cause variations in voxel intensity \autocite{noorApplicationDeepLearning2020}. 

It has been shown that 2D-CNN is widely used DL model and was found to be implemented more often than 3D-CNN in a review by \textcite{ebrahimighahnaviehDeepLearningDetect2020}. The only study using 2D-CNN was \textcite{nanniComparisonTransferLearning2020}. The authors claimed that transfer learning is easier on 2D-CNN since generic images used for training are mostly 2D. Nevertheless, by providing a method, whole brain scans could be used for their 2D-CNN which yielded an accuracy of \textit{90.20\%}. All other studies used 3D-CNN models with ROI \autocite[e.g.][]{liuMultimodelDeepConvolutional2020a, wangDenseCNNDenselyConnected2021, wenConvolutionalNeuralNetworks2020} or voxel as input \autocite[e.g.][]{wenConvolutionalNeuralNetworks2020, yeeConstructionMRIBasedAlzheimer2021,nanniComparisonTransferLearning2020}. Overall, the CNN studies reviewed used larger training data sets compared to SVM studies. Since training set size is shown to influence performance of the model \autocite[Wang, 2019, as cited in][]{ebrahimighahnaviehDeepLearningDetect2020} having larger training set can be beneficial and this is thought to be especially true for DL methods \autocite{vieiraUsingDeepLearning2017}. Researching the importance of training data size, \textcite{wenConvolutionalNeuralNetworks2020}, varied the size of training data (e.g using only baseline vs. longitudinal images) but no systematic changes in accuracy was found. The sufficient number is still a question of research \autocite{nanniComparisonTransferLearning2020}, a vague estimation is 1000 per class but depends on model complexity \autocite[Moradi, 2015, as cited in][]{nanniComparisonTransferLearning2020}. None of the reviewed studies had that much data. \textcite{wenConvolutionalNeuralNetworks2020} had the largest training set consisting of 1255 subjects from ADNI database. Techniques such as data augmentation and transfer learning can help with simulating larger training set, as well as reducing over fitting. \textcite{wangDenseCNNDenselyConnected2021} applied data augmentation which increased their original training set (\textit{n = 933}) six times (\textit{n = 5222}). A second study to use data augmentation was \textcite{yeeConstructionMRIBasedAlzheimer2021}. Transfer learning as other second technique was used by \textcite{wenConvolutionalNeuralNetworks2020} and \textcite{nanniComparisonTransferLearning2020}.
Four studies used 5-fold CV for validating their model. The accuracy of the reviewed studies varied between \textit{73.00\%} and \textit{95.70\%} with most of them being lower than \textit{90.00\%}. This result is not in line with previous reviews where most studies performed over \textit{90.00\%} on the classification task \autocite{ebrahimighahnaviehDeepLearningDetect2020, tanveerMachineLearningTechniques2020, joDeepLearningAlzheimer2019}. Even though model comparison is problematic \autocite{bron}, a possible explanation is provided by \textcite{wenConvolutionalNeuralNetworks2020}. In their literature review they conclude that half of their included studies may include data leakage. This form of bias that inflates accuracy is caused by using test data in the training part. The authors provide four main reason where data leakage can happen: "Wrong data split" (e.g. not splitting at subject level when longitudinal is provided),  "Late split" (e.g. feature selection, data augmentation after split), "Biased transfer learning" (e.g. overlap of source and target area) and "Absence of an independent test set" \autocite[][p. 4]{wenConvolutionalNeuralNetworks2020}. In this review \textcite{wangDenseCNNDenselyConnected2021} and \textcite{nanniComparisonTransferLearning2020} didn't use an independent test set which may open up the possibility of data leakage. The goal standard seems to be splitting the data set into three independent data sets: training, validating and testing set \autocite{introtostat, wenConvolutionalNeuralNetworks2020}. \textcite{wenConvolutionalNeuralNetworks2020} was the only study splitting the data set into three independent parts where as \textcite{yeeConstructionMRIBasedAlzheimer2021, liuMultimodelDeepConvolutional2020a} only used independent test set but validation was done on trianing set using cross-validation. Furthermore, \textcite{yeeConstructionMRIBasedAlzheimer2021} state that cross-validation was done at subject level to prevent data leakage since the authors were using longitudinal data. 
Like the studies using SVM, all CNNs were trained using ADNI database with independent testing on different databases only done by two studies \autocite[e.g.][]{wenConvolutionalNeuralNetworks2020,yeeConstructionMRIBasedAlzheimer2021}.   

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\subsection{Generalizability}
As \textcite{foundationof} point out generalizability is fundamental to \gls{ML} and even though cross-validation can be helpful estimating generalizability especially when data is sparse \autocite{introtostat}, testing on an independent database seems more appropriate in mimicking clinical practice where \gls{mri} protocols can vary without the model able to adapt \autocite{bron}. Studies like \textcite{liPredictionClinicalBiomarker2020}, that tested on independent data base, dropped \gls{svm}s accuracy by 12 percentage points (\textit{97.03\%} to \textit{84.85\%}), which indicates the problem of relying only on accuracy measures obtained from training data. Their testing set consisted of elderly Chinese subjects, whereas the model was trained with North American subjects. Even if the used gaussian kernel has been shown to be vulnerable to over fitting \autocite{gausover}, this decrease in performance could reflect a fundamental issue all studies share: the reliance on one database. Relying on one database can facilitate cross-comparisons between study but also overestimates accuracy \autocite{pellegriniMachineLearningNeuroimaging2018}. As discussed in \ref{database}, ADNI study launched an interest in standardizing \gls{mri} data sets around the world. This is one strategy of improving generalizability. Other strategies focusing on transparent comparison between trained models are challenges such as CDDementia \autocite{bron} and frameworks that make training and evaluation transparent such as \textcite{wenConvolutionalNeuralNetworks2020} provided. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Training Data}
Both methods included in this review are supervised methods. This means that they rely on ground truth values. In the task of classifying \gls{AD}, these ground truth values are the clinical diagnosis. As seen in the introduction, this diagnosis has changed in the past and still faces limitations. \textcite{jackNIAAAResearchFramework2018} describe the current \gls{AD} diagnosis as a diagnosis of dementia syndrome that can have multiple underlying causes with \gls{AD} only being one. With a definite \gls{AD} diagnosis only being possible post mortem and \textit{10\%} to \textit{30\%} clinically diagnosed with AD not displaying pathological changes post mortem, this ground truth problem is something that needs further research. Furthermore, it enhances the need to interpret the classifier's reason behind its decision. This is especially true for \gls{DL} models like \gls{cnn} since they are often described as "black boxes" \autocite{syaifullahMachineLearningDiagnosis2021}. \textcite{yeeConstructionMRIBasedAlzheimer2021} and \textcite{liuMultimodelDeepConvolutional2020a} were two studies using CNN that examined the factors contribution towards the decision made by their models. 
Furthermore, training data used for diagnosis are often elderly subjects where age related neurodegeneration could be a confounding variable. \textcite{liPredictionClinicalBiomarker2020} examined this possible problem by creating linear regression models for every selected feature so that the training subject could be compared with healthy individuals of same age. By controlling for age the authors could increase accuracy and sensitivity.   



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Clinical usage}
The CADDementia challenge provides participants with a data set (\textit{n = 30}) for training. The models of the participants are allowed to be trained with additional information (for example ADNI data) if desired. Since this challenge is focused on clinical usage, a small data set should mimic real world cases of little data available. Furthermore, the task for the classifier is not a binary one but a ternary one meaning the trained model should classify the subjects into \gls{AD}, \gls{mci} and \gls{nc}. Kl√∂ppel (2012) as cited in \textcite{bron} state that the potential for clinical usage of \gls{ML} models are 3 fold. Firstly, they can enhance diagnosis where there is a lack of expert knowledge that is needed to interpret \gls{mri} scans and other modalities. Secondly, the speed of diagnosis can be increased. Thirdly, subjects with similar patterns can be recruited for clinical trials.

For clinical usage it should be discussed if sensitivity or specificity of a model should be high. According to \textcite{BORST202015}, specificity reflects the percentage of people without the disease that are correctly excluded, or classified as not having the disease. Generally speaking, tests or models with high specificity models are good at ruling in a disease while high sensitivity are good at ruling out a disease \autocite{BORST202015}. Focusing on high specificity could minimize unnecessary burden that false positive classification lead to (e.g. treatment, medication, costs, psychological well being).

The question still remains if simple measurements such as age, gender and MMSE scores should be included into classification models. Multiple studies in this review have shown that including different modalities, especially MMSE scores, can lead to better performance  \autocite[see][]{khatriEfficientCombinationSMRI2020a,akramifardEmphasisLearningFeatures2020}. Still, a possible problem in this regard is circularity since accuracy depends on ground truths used to train the model. These ground truths, on the other hand, are clinical diagnosis that use neuropsychological tests such as MMSE. Nevertheless, since MRI is typically recommended following clinical evaluation \autocite{SCHELTENS2021}, having automated analysis that has shown to be able to outperform experts \autocite[e.g.][]{syaifullahMachineLearningDiagnosis2021} can be a clinically useful addition. 





